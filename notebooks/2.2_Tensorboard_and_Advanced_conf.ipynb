{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard 2\n",
    "\n",
    "This is a set of cells that gather examples from:\n",
    "- Learn Tensorflow 2.0, Singh & Manure, 2019\n",
    "- Tensorboard What's new in TensorFlow's visualization toolkit, TensorFlow Dev Summit, March, 2019\n",
    "\n",
    "### Example 1\n",
    "\n",
    "Basic example: doing a Sequential model, autogenerating the data base on np.random.sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73951549 0.35126237 0.43514608 0.59635915 0.26008277]\n",
      " [0.08427734 0.31638969 0.73797794 0.71426958 0.51056576]\n",
      " [0.89014269 0.40005075 0.35648198 0.04393161 0.42869708]\n",
      " ...\n",
      " [0.36070357 0.08176955 0.07583313 0.7109154  0.21631404]\n",
      " [0.89167466 0.45903446 0.71095811 0.42431557 0.47153313]\n",
      " [0.95396516 0.45590081 0.23032739 0.87325127 0.36933122]]\n"
     ]
    }
   ],
   "source": [
    "# Creating data\n",
    "X_train = (np.random.sample((10000,5)))\n",
    "y_train =  (np.random.sample((10000,1)))\n",
    "X_train.shape\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the tf.keras.Sequential model by stacking layers.\n",
    "model = tf.keras.models.Sequential([\n",
    "  #tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(256, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abelma/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 10000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 2s 213us/sample - loss: 0.0674 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 7.8399e-06 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 2.8972e-06 - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 1.5978e-06 - accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 1.5851e-06 - accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 1.5761e-06 - accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 1.1278e-06 - accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 1s 62us/sample - loss: 1.0761e-06 - accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 1.0612e-06 - accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 1s 55us/sample - loss: 1.0744e-06 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fec880a5278>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the estimator\n",
    "model.fit(X_train, y_train, epochs=10, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 21us/sample - loss: 9.9965e-07 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9.99648073593562e-07, 0.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6008\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7febf4172d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "Sequential model using MNIST dataset, training with Fit API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(x_train, y_train),(x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "#if keras.backend.image_data_format() == 'channels_first':\n",
    " #   shape_ord = (1, img_rows, img_cols)\n",
    "#else:  # channel_last\n",
    " #   shape_ord = (img_rows, img_cols, 1)\n",
    "\n",
    "#Normalizing data\n",
    "#X_train = X_train.reshape((X_train.shape[0],) + shape_ord)\n",
    "#X_test = X_test.reshape((X_test.shape[0],) + shape_ord)\n",
    "\n",
    "X_train = x_train.astype('float32')\n",
    "X_test = x_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(run_dir,hparams):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(hparams['num_units'], activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dropout(hparams['dropout_rate']),\n",
    "        tf.keras.layers.Dense(10,activation=tf.nn.softmax),\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=hparams['optimizer'],\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    model.fit(X_train,y_train,\n",
    "             validation_data=(X_test,y_test),\n",
    "             epochs=3,\n",
    "             callbacks=[tf.keras.callbacks.TensorBoard(run_dir + \"/keras\")])\n",
    "    \n",
    "    scores = model.evaluate(X_test,y_test)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.4575 - accuracy: 0.8669 - val_loss: 0.2218 - val_accuracy: 0.9382\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2654 - accuracy: 0.9214 - val_loss: 0.1674 - val_accuracy: 0.9494\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 2s 41us/sample - loss: 0.2269 - accuracy: 0.9313 - val_loss: 0.1468 - val_accuracy: 0.9568\n",
      "10000/10000 [==============================] - 0s 17us/sample - loss: 0.1468 - accuracy: 0.9568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14675281562581657, 0.9568]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_model('logs/sample',{'num_units':32, 'dropout_rate': 0.2, 'optimizer': 'adam'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6007\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fec6b9bef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/sample/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3\n",
    "\n",
    "The same example 2 but using Tensorboard plugins for hyper parameters tuning and some new stuff for experimental summary configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api_pb2\n",
    "from tensorboard.plugins.hparams import summary as hparams_summary\n",
    "from google.protobuf import struct_pb2\n",
    "from tensorboard.plugins.hparams import api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_units_list =[16,32]\n",
    "dropout_rate_list= [0.1, 0.2]\n",
    "optimizer_list = ['adam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_summary(num_units_list, dropout_rate_list, optimizer_list):\n",
    "    \n",
    "    num_units_list_val = struct_pb2.ListValue()\n",
    "    num_units_list_val.extend(num_units_list)\n",
    "    dropout_rate_list_val = struct_pb2.ListValue()\n",
    "    dropout_rate_list_val.extend(dropout_rate_list)\n",
    "    optimizer_list_val = struct_pb2.ListValue()\n",
    "    optimizer_list_val.extend(optimizer_list)\n",
    "    return hparams_summary.experiment_pb(\n",
    "        #List our hyperparameters\n",
    "        hparam_infos=[\n",
    "            api_pb2.HParamInfo(name = 'num_units',\n",
    "                              display_name = '# Units',\n",
    "                              type = api_pb2.DATA_TYPE_FLOAT64,\n",
    "                              domain_discrete=num_units_list_val),\n",
    "            api_pb2.HParamInfo(name = 'dropout_rate',\n",
    "                              display_name = 'Dropout Rate',\n",
    "                              type = api_pb2.DATA_TYPE_FLOAT64,\n",
    "                              domain_discrete=dropout_rate_list_val),\n",
    "            api_pb2.HParamInfo(name = 'optimizer',\n",
    "                              display_name = 'Optimizer',\n",
    "                              type = api_pb2.DATA_TYPE_STRING,\n",
    "                              domain_discrete=optimizer_list_val)\n",
    "        ],\n",
    "        #List our metrics\n",
    "        metric_infos=[\n",
    "            api_pb2.MetricInfo(\n",
    "                name=api_pb2.MetricName(\n",
    "                    tag = 'accuracy'),\n",
    "                display_name='Accuracy'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "exp_summary = create_experiment_summary(num_units_list, dropout_rate_list, optimizer_list)\n",
    "logdir_writer = tf.compat.v2.summary.create_file_writer('logs/hparam_tuning')\n",
    "with logdir_writer.as_default():\n",
    "    tf.compat.v2.summary.experimental.write_raw_pb(exp_summary.SerializeToString(), step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    writer = tf.summary.create_file_writer(run_dir)\n",
    "    summary_start = hparams_summary.session_start_pb(hparams=hparams)\n",
    "    \n",
    "    with writer.as_default():\n",
    "        tf.compat.v2.summary.experimental.write_raw_pb(exp_summary.SerializeToString(), step=0)\n",
    "        loss, accuracy = train_test_model(run_dir, hparams)\n",
    "        \n",
    "        tf.summary.scalar('accuracy', accuracy, step=0, description='The Accuracy')\n",
    "        summary_end = hparams_summary.session_end_pb(api_pb2.STATUS_SUCCESS)\n",
    "        tf.compat.v2.summary.experimental.write_raw_pb(summary_end.SerializeToString(), step=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6009\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fec282b1160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Running training session 1\n",
      "{'num_units': 16, 'dropout_rate': 0.1, 'optimizer': 'adam'}\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.5698 - accuracy: 0.8277 - val_loss: 0.2793 - val_accuracy: 0.9212\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.3520 - accuracy: 0.8931 - val_loss: 0.2348 - val_accuracy: 0.9310\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 0.3095 - accuracy: 0.9068 - val_loss: 0.2130 - val_accuracy: 0.9360\n",
      "10000/10000 [==============================] - 0s 25us/sample - loss: 0.2130 - accuracy: 0.9360\n",
      "---- Running training session 2\n",
      "{'num_units': 16, 'dropout_rate': 0.2, 'optimizer': 'adam'}\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 4s 61us/sample - loss: 0.6427 - accuracy: 0.8004 - val_loss: 0.2946 - val_accuracy: 0.9171\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.4241 - accuracy: 0.8688 - val_loss: 0.2633 - val_accuracy: 0.9241\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.3880 - accuracy: 0.8798 - val_loss: 0.2354 - val_accuracy: 0.9324\n",
      "10000/10000 [==============================] - 0s 23us/sample - loss: 0.2354 - accuracy: 0.9324\n",
      "---- Running training session 3\n",
      "{'num_units': 32, 'dropout_rate': 0.1, 'optimizer': 'adam'}\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.4225 - accuracy: 0.8778 - val_loss: 0.2098 - val_accuracy: 0.9385\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.2315 - accuracy: 0.9320 - val_loss: 0.1713 - val_accuracy: 0.9502\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1898 - accuracy: 0.9439 - val_loss: 0.1429 - val_accuracy: 0.9585\n",
      "10000/10000 [==============================] - 0s 24us/sample - loss: 0.1429 - accuracy: 0.9585\n",
      "---- Running training session 4\n",
      "{'num_units': 32, 'dropout_rate': 0.2, 'optimizer': 'adam'}\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.4757 - accuracy: 0.8602 - val_loss: 0.2217 - val_accuracy: 0.9358\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.2794 - accuracy: 0.9166 - val_loss: 0.1781 - val_accuracy: 0.9472\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.2368 - accuracy: 0.9297 - val_loss: 0.1532 - val_accuracy: 0.9548\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 0.1532 - accuracy: 0.9548\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for num_units in num_units_list:\n",
    "    for dropout_rate in dropout_rate_list:\n",
    "        for optimizer in optimizer_list:\n",
    "            hparams = {'num_units': num_units, 'dropout_rate': dropout_rate, 'optimizer': optimizer}\n",
    "            print('---- Running training session %d' % (session_num+1))\n",
    "            print(hparams)\n",
    "            run_name = \"run-%d\" % session_num\n",
    "            run('logs/hparam_tuning/'+run_name, hparams)\n",
    "            session_num +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclussions\n",
    "\n",
    "The codes for tensorboard are complex, the configuration for hyperparameter tunning is very useful but must be studied in deep.\n",
    "\n",
    "The examples given in March of 2019 were deprecated in April due to the fast rate of changes of tensorboard library 2.x.\n",
    "\n",
    "This refactorization of March 2019 codes has some problems because isn't show the the rows in \"Table View\", therefore the graphics in \"Parallel Cordinate Views\" are not shown either.\n",
    "\n",
    "### Recomendation\n",
    "\n",
    "* It is a good practice to follow the changes in github, some practical examples, when you do diff, are shown graphically, so you can see the old version and how to implement the same example in the new version.\n",
    "\n",
    "* The recommended url is [https://github.com/tensorflow/tensorboard](https://github.com/tensorflow/tensorboard.git)\n",
    "\n",
    "* ESPECIAL recommendation to the jupyter notebook [https://github.com/tensorflow/tensorboard/docs/hyperparameter_tuning_with_hparams](https://github.com/tensorflow/tensorboard/blob/master/docs/hyperparameter_tuning_with_hparams.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
