TensorFlow Reinforcemente Learning
***************************************

This project is the demo, training, toy, whatever for the preparation of
Wittylytic's DataSci Team on the building of a Recommendation System 

Objectives
===========

* Test tensorflow 2.0 and tensorboard
* Test small datasets and performance of classical supervised and
unsupervised algorithm.
* Test serving a model as a microservices.
* Test tensorflow-serving with the same models.
* Test the AWS platform to serv the same microservices.
* Create metrics to evaluate and suggest technical issues to the 
infraestructure team and the...
* IT Strategy Consulting clients
* To document the math mecanism that sustain every kind of NN used to 
study-remember-reuse faster those math in new Apps.


Examples
============

* [Example 1]()

Notebooks
============

* [Notebook 1]()

Important Links
=================

* [AWS Free Tier](https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc)
* [sorice personal DL Course using tensorflow 1.13](https://github.com/sorice/tensorflow1.13.git)
* [Notebooks Book "Deep Learning with Python"](https://github.com/fchollet/deep-learning-with-python-notebooks.git)
* [Deep Learning Keras-Tensorflow tutorial](https://github.com/leriomaggio/deep-learning-keras-tensorflow)
* [TensorFlow Examples](https://github.com/aymericdamien/TensorFlow-Examples)
* [DL with Keras - Tensorflow](https://github.com/leriomaggio/deep-learning-keras-tensorflow.git)

Future Readings
=================

- Fix this Lack of parallelization of LSTM
- Learning Long Term Dependency remains open
- [Attention concept](https://arxiv.org/abs/1409.0473)
- [Transformer NN Architecture](https://arxiv.org/abs/1706.03762)
- [BERT](https://arxiv.org/abs/1810.04805)
- Hierarchical Temporal Memory (HTM)
- Cascade Forests
- Capsule Networks (CapsNets),

[Advances in NLP, Report of December 25 2019](https://towardsdatascience.com/recent-advancements-in-nlp-2-2-df2ee75e189)

Resources
* [Visualization of RNN, Attention, and others](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
* [Building Transformers from scratch](https://github.com/keitakurita/Practical_NLP_in_PyTorch/blob/master/deep_dives/transformer_from_scratch.ipynb)
* [Library for using various transformers architectures](https://github.com/huggingface/transformers)
* [cheat sheets for AI, NN, ML, DL,, Big Data](https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463)
* [more cheat sheets](https://www.datasciencecentral.com/profiles/blogs/large-collection-of-neural-networks-ml-numpy-pandas-matplotlib-sc)
